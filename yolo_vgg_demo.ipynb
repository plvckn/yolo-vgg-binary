{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolo-vgg-demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcaagrgFkmYUr/D//+MT2r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plvckn/yolo-vgg-binary/blob/main/yolo_vgg_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvdATjZwzqWj"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnHw5JjazZ3P",
        "outputId": "d4f052ab-91a9-43bd-cbee-87f35f1b9b66"
      },
      "source": [
        "# Mount to drive and cd to darkflow repo\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "foldername = 'darkflow'\n",
        "sys.path.append(f'/content/drive/My Drive/custom-yolo/{foldername}')\n",
        "%cd /content/drive/My\\ Drive/custom-yolo/$foldername\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/custom-yolo/darkflow\n",
            "bin\t\t      ckpt\tlabels-coco.txt  sample_img  video.avi\n",
            "binaryYoloLabels.txt  darkflow\tlabels.txt\t setup.py    yolov2-voc-1c.cfg\n",
            "build\t\t      demo\tLICENSE\t\t test\n",
            "built_graph\t      demo.gif\tpreview.png\t test_img\n",
            "cfg\t\t      flow\tREADME.md\t train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0QySvmTziwi"
      },
      "source": [
        "# Packages\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "from os.path import join\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltu1l-TIthrT"
      },
      "source": [
        "# Define paths to save video frames, detection and classification results\n",
        "SAVE_FOLDER = 'frames_test3' \n",
        "if not os.path.exists(f'demo/{SAVE_FOLDER}'): \n",
        "  os.makedirs(f'demo/{SAVE_FOLDER}') \n",
        "if not os.path.exists(f'demo/processed/{SAVE_FOLDER}'):\n",
        "  os.makedirs(f'demo/processed/{SAVE_FOLDER}')\n",
        "imgdir = f'demo/{SAVE_FOLDER}'\n",
        "out_name = 'test-02t-bigcrops-keep-ratio' # processed video filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rWFG7UsoC0s"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZzX6ihloEGu"
      },
      "source": [
        "# Various utility and helper functions\n",
        "\n",
        "def deserialize_json(obj_file):\n",
        "  f = open(obj_file, 'r')\n",
        "  json_str = json.loads(f.read())\n",
        "  json_obj = json.loads(json_str[1:-1].replace(\"\\'\", \"\\\"\"))\n",
        "  return json_obj\n",
        "\n",
        "def read_img(img_id):\n",
        "  img_id = img_id.replace('.json', '.jpg')\n",
        "  img = cv2.imread(join(imgdir, img_id))\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  return img\n",
        "\n",
        "def resize(img, keep_ratio=False):\n",
        "  return np.expand_dims(tf.image.resize(img, (224,224), preserve_aspect_ratio=keep_ratio).numpy(), 0)\n",
        "\n",
        "def get_crop(obj, img):\n",
        "  xmin = obj['topleft']['x']\n",
        "  ymin = obj['topleft']['y']\n",
        "  xmax = obj['bottomright']['x']\n",
        "  ymax = obj['bottomright']['y']\n",
        "  return img[ymin:ymax,xmin:xmax,:]\n",
        "\n",
        "def get_bigger_crop(obj, img, ratio=0.1):\n",
        "  y, x, _ = img.shape\n",
        "  xmin = obj['topleft']['x']\n",
        "  ymin = obj['topleft']['y']\n",
        "  xmax = obj['bottomright']['x']\n",
        "  ymax = obj['bottomright']['y']\n",
        "  #crop more of the picture without going over the edges\n",
        "  xextend = int((xmax-xmin)*ratio)\n",
        "  yextend = int((ymax-ymin)*ratio)\n",
        "  xmin = max(xmin-xextend,0)\n",
        "  ymin = max(ymin-yextend,0)\n",
        "  xmax = min(xmax+xextend,x)\n",
        "  ymax = min(ymax+yextend,y)\n",
        "  return img[ymin:ymax,xmin:xmax,:]\n",
        "\n",
        "def draw_boxes(objects, image):\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  for obj in objects:\n",
        "      tl = (obj['topleft']['x'], obj['topleft']['y'])\n",
        "      br = (obj['bottomright']['x'], obj['bottomright']['y'])\n",
        "      predscore = obj['predscore']\n",
        "      mismatch = float(predscore) > 0.5 # yolo detected lemon, but classifier predicted not lemon\n",
        "      if not mismatch:\n",
        "        label = obj['label']\n",
        "        conf = obj['confidence']\n",
        "        text = '{}: {:.0f}%'.format(label, conf * 100)\n",
        "        image = cv2.rectangle(image, tl, br, color=(0,0,0), thickness=5)\n",
        "        image = cv2.putText(image, text, tl, cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 0), 2)\n",
        "      else:\n",
        "        image = cv2.rectangle(image, tl, br, color=(0,0,255), thickness=5)\n",
        "        text = 'CLASSIFIER MISMATCH'\n",
        "        image = cv2.putText(image, text, tl, cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 2)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuvaNmYZzr-i"
      },
      "source": [
        "### Load yolo model (detection) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prwgKaYRzkU8"
      },
      "source": [
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-gpu==1.14 #darkflow needs TF 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cszEtaHuzl_y"
      },
      "source": [
        "from darkflow.net.build import TFNet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okVKho7RzwHO"
      },
      "source": [
        "# Detection config\n",
        "DETECTION_THRESHOLD = 0.2\n",
        "options = {\n",
        "    \"metaLoad\": \"built_graph/yolov2-voc-1c.meta\",\n",
        "    \"pbLoad\": \"built_graph/yolov2-voc-1c.pb\",\n",
        "    \"threshold\": DETECTION_THRESHOLD,\n",
        "    \"gpu\": 0.9\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb1Jr76M0JAK"
      },
      "source": [
        "tfnet = TFNet(options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSbH0CiP0Vii"
      },
      "source": [
        "### Get detections from video (YOLO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mwo-bLq0XeE"
      },
      "source": [
        "'''\n",
        "Steps:\n",
        "1. Iterate through the video frame by frame\n",
        "2. Pass frame to object detection model and get bounding box results as python dict\n",
        "3. Save the frame as .jpg and the corresponsing python dict with results as .json file\n",
        "'''\n",
        "SAVE_EVERY = 1 # save every n'th video frame as an image and pass it to yolo\n",
        "video_path = 'demo/live_video/video3.mp4'\n",
        "vidcap = cv2.VideoCapture(video_path)\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "while success:     \n",
        "  success, image = vidcap.read()\n",
        "  if success:\n",
        "    if count % SAVE_EVERY == 0:\n",
        "      image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
        "      results = tfnet.return_predict(image)\n",
        "      cv2.imwrite(f'demo/{SAVE_FOLDER}/frame{count}.jpg', image)\n",
        "      with open(f'demo/{SAVE_FOLDER}/frame{count}.json', 'w') as f:\n",
        "        json.dump(json.dumps(str(results)), f)\n",
        "  count += 1\n",
        "vidcap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S7S09vAUVFg"
      },
      "source": [
        "### Classify image crops (VGG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xziRRYkCVxiq"
      },
      "source": [
        "#!pip install tensorflow==2.6.0 # needs to be TF 2.X for classification, restart runtime after installing from 1.X to 2.X and reimport packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANs9NFTvjCrU"
      },
      "source": [
        "# Classification config\n",
        "keep_aspect_ratio = False\n",
        "bigger_crops = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX9yi1eCg762"
      },
      "source": [
        "# Get json files\n",
        "json_files = sorted(\n",
        "    glob.glob(f'demo/{SAVE_FOLDER}/*.json'),\n",
        "    key = lambda filepath: int(os.path.basename(filepath).split('.')[0].split('frame')[1])\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model_path = r'/content/drive/My Drive/custom-yolo/lemon_binary_classification/vgg16_binary_base'\n",
        "model = load_model(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tpHrBI0R5Wq",
        "outputId": "ab55aa79-255b-4458-bc06-d9d2b15dc7bb"
      },
      "source": [
        "'''\n",
        "Steps:\n",
        "1. Iterate over every json file containing yolo detected bounding-boxes.\n",
        "2. Each json file is a list of detected objects in one image frame, containing object coordinates (topleft xy, bottomright xy), confidence, label.\n",
        "3. Extract crops from the image based on json data.\n",
        "4. Pass crops to a binary vgg for classification of lemon/not lemon.\n",
        "5. Draw bounding boxes based on the results of both yolo and vgg:\n",
        "  a. If yolo detects a lemon and vgg classifies the crop as 'lemon' draw a regular bounding box displaying class and confidence.\n",
        "  b. If yolo detects a lemon but vgg classifies 'not lemon' draw a false bounding box indicating classifier and detector mismatch.\n",
        "6. Save updated results in .jpg (for images) and .json (for detection/classification data)\n",
        "'''\n",
        "frame_count = 0\n",
        "for json_file in json_files:\n",
        "  img_id = os.path.basename(json_file)\n",
        "  img = read_img(img_id)\n",
        "  json_obj = deserialize_json(json_file)\n",
        "  for obj in json_obj:\n",
        "    if not bigger_crops:\n",
        "      obj_crop = get_crop(obj, img)\n",
        "    else:\n",
        "      obj_crop = get_bigger_crop(obj, img)\n",
        "    obj_crop = resize(obj_crop, keep_ratio=keep_aspect_ratio)\n",
        "    result = model.predict(preprocess_input(obj_crop.copy()))\n",
        "    predscore = result[0][0] \n",
        "    predlabel = 'lemon' if predscore <= 0.5 else 'not lemon'\n",
        "    obj.update({'predscore': str(round(predscore,2)), 'predlabel': predlabel})\n",
        "  img = draw_boxes(json_obj, img)\n",
        "  cv2.imwrite(f'demo/processed/{out_name}/frame{frame_count}.jpg', img)\n",
        "  with open(f'demo/processed/{out_name}/frame{frame_count}.json', 'w') as f:\n",
        "    json.dump(json_obj, f)\n",
        "  frame_count += 1\n",
        "  if frame_count % 25 == 0:\n",
        "    print(f'processed {frame_count} frames')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 25 frames\n",
            "processed 50 frames\n",
            "processed 75 frames\n",
            "processed 100 frames\n",
            "processed 125 frames\n",
            "processed 150 frames\n",
            "processed 175 frames\n",
            "processed 200 frames\n",
            "processed 225 frames\n",
            "processed 250 frames\n",
            "processed 275 frames\n",
            "processed 300 frames\n",
            "processed 325 frames\n",
            "processed 350 frames\n",
            "processed 375 frames\n",
            "processed 400 frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeF_oIh8uGUp"
      },
      "source": [
        "### Recreate video from processed frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pBceoOIuJGQ"
      },
      "source": [
        "# Define video capture settings\n",
        "out = out_name+'.avi'\n",
        "outpath = join('demo/processed', out)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "height, width, depth = cv2.imread(f'{imgdir}/frame0.jpg').shape\n",
        "videoWriter = cv2.VideoWriter(outpath, fourcc, 30, (width,height))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2LqDieWu69i"
      },
      "source": [
        "# Get preprocessed frames\n",
        "frames = glob.glob(f'demo/processed/{out_name}/*.jpg')\n",
        "frames = sorted(frames, key = lambda filepath: int(os.path.basename(filepath).split('.')[0].split('frame')[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSWHBl5vwlpp"
      },
      "source": [
        "# Assemble a video from frames\n",
        "for frame in frames:\n",
        "  im = cv2.imread(frame)\n",
        "  videoWriter.write(im)\n",
        "videoWriter.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}